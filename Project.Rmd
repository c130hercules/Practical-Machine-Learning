---
title: "Practical Machine Learning Project"
author: "Nicholas Trankle"
date: "02/07/2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
This document contains the work as required by the Practical Machine Learning course on Coursera. 

The project background brief was given as follows:

"Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways."

## Project Brief

The **aim** of the project is to use training data to build a model that will classify the manner in which the exercise was performed. Using R's **caret** package, we will build two models, a decision tree model and a random forest model. Since the size of the data is so large, the training data will be partitioned into a 60% model training set and a 40% model testing set. This will suffice as model cross validation. We will then report on the expected out of sample error. The more accurate model will then be chosen and subjected to the test data provided to classify the manner in which the exercises were performed.

The project will be divided into a number of steps which will be summarised as follows:
1.) Cleaning the data.
2.) Dividing the training data into a training and testing set.
3.) Create a model using the decision tree algorithm and test it's capabilities.
4.) Create another model using the random forest algorithm and test it's capabilities.
5.) Use the better of the two models to make predictions on the test data.

```{r}
library("caret")
library("rpart")
library("rpart.plot")
library("RColorBrewer")
library("rattle")
library("randomForest")
ProjTrain = read.csv("C:/Users/gct1/Desktop/pml-training.csv")
ProjTest = read.csv("C:/Users/gct1/Desktop/pml-testing.csv")
```

The data have been downloaded and stored into dataframes ProjTrain and ProjTest.

## 1.) Cleaning the data

After having analysed the raw data, there are a three pertinent observations that can be made.

1.) The first column ("X") seems to be an ID column. In other words, it simply acts as a reference to a specific entry. Although this may be useful in some instances, it should not be included in the data sent to the model as it has no influence on the data and the manner (classe) in which the movement was done.

```{r}
ProjTrain <- ProjTrain[c(-1)]
dim(ProjTrain) # checking to see that a (the ID) column has been removed.
```

2.) The data are analysed to check if there are any variables that have near zero variance. R's function NearZeroVariance() provides a useful summary, indicating which variables could be excluded from that model. These variables are then excluded from the dataframe which will be fed into the model.

```{r}
NZV <- nearZeroVar(ProjTrain, saveMetrics = T)
NZV <- NZV[NZV$nzv == 1,]
NZV_RowNames <- row.names(NZV)
ProjTrain <- subset(ProjTrain, select = !names(ProjTrain) %in% NZV_RowNames)
dim(ProjTrain) # checking the dimensions of ProjTrain
```

3.) Many of the variables contain a lot of NA entries. These variables could also render the model to be inaccurate. As such, a decision is made to exclude variables which contain more than 60% NA's.

```{r}
temp <- ProjTrain  # a necessary step as we will remove columns which makes loop indexing complicated
for(i in 1:length(ProjTrain)){   # loop for each column
  if(sum(is.na(ProjTrain[,i]))/nrow(ProjTrain) >= 0.6){   # evaluates if more than 60% NA's
    for(j in 1:length(temp)){
      if(length(grep(names(ProjTrain[i]), names(temp)[j])) == 1){   # removes the column off of temp
        temp <- temp[, -j]
      }
    }
  }
}
ProjTrain <- temp   # updating ProjTrain after the applicable variables have been removed.
dim(ProjTrain)   # Checking ProjTrain to see how many variables are left
```
Since the training data has been cleaned for the model, we can also remove these columns from the test data. The remaining column names in modTraining will be extracted and applied to ProjTest.

```{r}
modTraining_colNames <- colnames(ProjTrain)
# Since ProjTest does not include the column classe (it is replaced with the column problem_id), we will remove the classe column from the colNames variable defined above.
modTraining_colNames <- colnames(ProjTrain[,-58])
TestData <- ProjTest[modTraining_colNames]
dim(TestData)   # Check that TestData has been correctly modified
```

## 2.) Divide training data into training and testing sets

As per standard best practices, the provided training data is partitioned into model training and model testing sets with a 60:40 ratio.

```{r}
inTrain <- createDataPartition(y = ProjTrain$classe, p=0.6, list=FALSE)
modTraining <- ProjTrain[inTrain,]
modTesting <- ProjTrain[-inTrain,]
dim(modTraining)
dim(modTesting)
```

To ensure that there are no issues with the training versus testing data, the test data is coerced to be the same as the training data.

```{r}
for(i in 1: length(TestData)){
  for(j in 1:length(modTraining)){
    if(length(grep(names(modTraining[i]), names(TestData)[j])) == 1){
      class(TestData[j]) <- class(modTraining[i])
    }
  }
}
```

## 3.) Create a model using the decision tree algorithm and test it's capabilities

```{r}
dtModel <- rpart(classe ~ ., data = modTraining, method = "class")
```

A plot of the Decision Tree is given below:

```{r}
fancyRpartPlot(dtModel)
```

The Decision Tree model is tested on the partitioned data modTesting and the results are given.

```{r}
dtPredict <- predict(dtModel, modTesting, type = "class")
```
```{r}
confusionMatrix(table(dtPredict, modTesting$classe))
```

As can be seen by the results, the out of sample accuracy of the decision tree model is 89.23%. The 95% confidence interval of this accuracy is 88.52% to 89.91%.

## 4.) Create another model using the random forest algorithm and test it's capabilities.

```{r}
rfModel <- randomForest(as.factor(classe) ~ ., data = modTraining)
rfPredict <- predict(rfModel, modTesting, type = "class")
confusionMatrix(table(rfPredict, modTesting$classe))
```

The results of the random forest model indicate an out of sample accuracy of 99.87%, with a 95% confidence interval of 99.77 to 99.94%. Since this model is more accurate than the decision tree model, it is selected as the model of choice to evaluate the TestData.


## 5.) Use the better of the two models to make predictions on the test data.

The random forest model is used to predict the class of the provided test data.

```{r}
TestPredictions <- predict(rfModel,ProjTest, type = "class")
TestPredictions